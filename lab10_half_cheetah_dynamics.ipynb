{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199af124",
   "metadata": {},
   "source": [
    "\n",
    "# HalfCheetah: Learn a Dynamics Model from Random Rollouts (Then Validate It)\n",
    "\n",
    "**Goal:** In this notebook you'll (1) collect random experience tuples \\((s_t, a_t, r_t, s_{t+1})\\) from `HalfCheetah-v4`, (2) train a neural network to predict **state deltas** \\(\\Delta s = s_{t+1}-s_t\\), and (3) **validate** the model with one-step and multi-step (open-loop) rollouts.\n",
    "\n",
    "This mirrors the first phase of model-based control (e.g., MPPI): learn a model offline, then use it for planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3392de",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Requirements\n",
    "\n",
    "- Python 3.9+\n",
    "- PyTorch `>= 1.10`\n",
    "- Gymnasium `>= 0.29`\n",
    "- MuJoCo with `HalfCheetah-v4` (install `mujoco` and `gymnasium[mujoco]`)\n",
    "\n",
    "```bash\n",
    "pip install \"gymnasium[mujoco]\" mujoco torch matplotlib\n",
    "```\n",
    "\n",
    "**Try and understand what RunningNormalizer does.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1213bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "def to_t(x): \n",
    "    return th.as_tensor(x, dtype=th.float32)\n",
    "\n",
    "def fanin_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        bound = 1.0 / math.sqrt(m.weight.size(1))\n",
    "        nn.init.uniform_(m.weight, -bound, +bound)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "class RunningNormalizer:\n",
    "    \"\"\"Feature-wise running mean/std (Welford).\"\"\"\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        self.dim = dim\n",
    "        self.count = 0\n",
    "        self.mean = np.zeros(dim, dtype=np.float64)\n",
    "        self.M2   = np.zeros(dim, dtype=np.float64)\n",
    "        self.eps  = eps\n",
    "\n",
    "    def update(self, x: np.ndarray):\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1: x = x[None, :]\n",
    "        for v in x:\n",
    "            self.count += 1\n",
    "            d = v - self.mean\n",
    "            self.mean += d / self.count\n",
    "            d2 = v - self.mean\n",
    "            self.M2 += d * d2\n",
    "\n",
    "    @property\n",
    "    def var(self):\n",
    "        if self.count < 2: return np.ones(self.dim, dtype=np.float64)\n",
    "        return self.M2 / (self.count - 1 + 1e-12)\n",
    "\n",
    "    @property\n",
    "    def std(self): \n",
    "        return np.sqrt(self.var + self.eps)\n",
    "\n",
    "    def normalize(self, x): return (x - self.mean) / self.std\n",
    "    def denormalize(self, x): return x * self.std + self.mean\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); th.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b5ce5",
   "metadata": {},
   "source": [
    "\n",
    "## Initializing Environment and Figure Out Observation Structure\n",
    "\n",
    "`HalfCheetah-v4` exposes observations as `[qpos[1:], qvel[:]]`. The forward velocity is `qvel[0]`, which sits at index `len(qpos[1:])` inside the observation vector. We'll extract that index for later validation/plots. :::: This is important for planning, if we want to know what each state represents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de69613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim: 17 act_dim: 6 qvel_start: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "set_seed(42)\n",
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "act_low = env.action_space.low\n",
    "act_high = env.action_space.high\n",
    "\n",
    "# Find start index of qvel inside obs = [qpos[1:], qvel[:]]\n",
    "nq = env.unwrapped.model.nq\n",
    "qvel_start = int(nq - 1)\n",
    "print(\"obs_dim:\", obs_dim, \"act_dim:\", act_dim, \"qvel_start:\", qvel_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071b0bb",
   "metadata": {},
   "source": [
    "## ðŸ§© Task 1: Prepare the Replay Buffer\n",
    "**Goal:** Store transitions \\((s_t, a_t, s_{t+1})\\) and return training pairs \\((x, y) = (s_t, a_t, s_{t+1} - s_t)\\).\n",
    "\n",
    "**Instructions:**\n",
    "- Implement `add()` to record transitions.\n",
    "- Add a `sample()` method to randomly sample batch of certain size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08628879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Replay:\n",
    "    def __init__(self, obs_dim, act_dim, capacity=300_000):\n",
    "        self.obs = np.zeros((capacity, obs_dim), dtype=np.float32)\n",
    "        self.act = np.zeros((capacity, act_dim), dtype=np.float32)\n",
    "        self.nxt = np.zeros((capacity, obs_dim), dtype=np.float32)\n",
    "        self.rew = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.term = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.ptr = 0; self.size = 0; self.cap = capacity\n",
    "\n",
    "    def add(self, s, a, r, sp, term):\n",
    "        i = self.ptr\n",
    "        self.obs[i] = s; self.act[i] = a; self.rew[i] = r\n",
    "        self.nxt[i] = sp; self.term[i] = term\n",
    "        self.ptr = (i + 1) % self.cap\n",
    "        self.size = min(self.size + 1, self.cap)\n",
    "\n",
    "    def sample(self, batch):\n",
    "        idx = np.random.randint(0, self.size, size=batch)\n",
    "        return self.obs[idx], self.act[idx], self.rew[idx], self.nxt[idx], self.term[idx]\n",
    "\n",
    "replay = Replay(obs_dim, act_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374692ef",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2. Collect Random Rollouts\n",
    "\n",
    "- Gather random actions for a number of steps to create our training dataset. Collect data for 100000 steps. \n",
    "- Call the function and fill the replay buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d7b840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksubh\\anaconda3\\envs\\mquad_cuda\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected transitions: 200000 example return: -55736.48962909418\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_random(env, replay, steps=10000, seed=42):\n",
    "    s, _ = env.reset(seed=seed)\n",
    "    ret = 0.0\n",
    "    for _ in range(steps):\n",
    "        a = env.action_space.sample()\n",
    "        sp, r, term, trunc, _ = env.step(a)\n",
    "        replay.add(s, a, r, sp, float(term or trunc))\n",
    "        ret += r\n",
    "        s = sp\n",
    "        if term or trunc:\n",
    "            s, _ = env.reset()\n",
    "    return ret\n",
    "\n",
    "total_return = collect_random(env, replay, steps=200000, seed=42)\n",
    "print(\"Collected transitions:\", replay.size, \"example return:\", total_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7689c7f",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3. Update normalizers from the collected random data in the replay buffer\n",
    "\n",
    "We normalize inputs (`[s,a]`) and targets (`Î”s = s' - s`) for stable training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39332ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizers ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obs_norm = RunningNormalizer(obs_dim)\n",
    "inp_norm = RunningNormalizer(obs_dim + act_dim)\n",
    "targ_norm = RunningNormalizer(obs_dim)\n",
    "\n",
    "def update_normalizers_from_buffer(replay, maxN=50000):\n",
    "    n = min(replay.size, maxN)\n",
    "    idx = np.random.choice(replay.size, size=n, replace=False)\n",
    "    obs = replay.obs[idx]; act = replay.act[idx]; nxt = replay.nxt[idx]\n",
    "    delta = nxt - obs\n",
    "    xin = np.concatenate([obs, act], axis=1)\n",
    "    obs_norm.update(obs)\n",
    "    inp_norm.update(xin)\n",
    "    targ_norm.update(delta)\n",
    "\n",
    "update_normalizers_from_buffer(replay)\n",
    "print(\"Normalizers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566164a4",
   "metadata": {},
   "source": [
    "\n",
    "## Defining the Neural Dynamics Model\n",
    "\n",
    "We predict **normalized** `Î”s` from **normalized** `[s, a]`.\n",
    "NN parameters: \n",
    "\n",
    "- initialize a deterministic NN with a ExponentialLR sceduler( that decays the learning rate with epoch)\n",
    "- width = 200, depth = 3, lr = 1e-3, weight_decay - 1e-5, gamma for scheduler = 0.8\n",
    "- These are a starting point but not the best parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d3eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DetMLP(nn.Module):\n",
    "    \"\"\"Predicts Î”state deterministically.\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, width=200, depth=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for _ in range(depth):\n",
    "            layers += [nn.Linear(last, width), nn.ReLU()]\n",
    "            last = width\n",
    "        layers += [nn.Linear(last, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.apply(fanin_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "in_dim = obs_dim + act_dim\n",
    "out_dim = obs_dim\n",
    "model = DetMLP(in_dim, out_dim, width=200, depth=3)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3db88",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4. Train the Model\n",
    "\n",
    "We minimize MSE between predicted normalized `Î”s` and target normalized `Î”s`.\n",
    "\n",
    "- Train in batches, keep the batch size 256\n",
    "- Use a learning rate scheduler that decays the learning rate as training progresses. You may use the pytorch utility. See how the learning rate decays with each epoch. \n",
    "- Train for 30 epochs and plot the training curve. Loss vs epoch. \n",
    "- Find the best parameters(defined in the previous block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_batch(replay, batch):\n",
    "    s, a, _, sp, _ = replay.sample(batch)\n",
    "    delta = sp - s\n",
    "    xin = np.concatenate([s, a], axis=1)\n",
    "    xin_n = to_t(inp_norm.normalize(xin))\n",
    "    delta_n = to_t(targ_norm.normalize(delta))\n",
    "    return xin_n, delta_n\n",
    "\n",
    "def train_model(model, replay, epochs=30, batch_size=256):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        s = 0.0\n",
    "        iters = max(1, replay.size // batch_size)\n",
    "        for _ in range(iters):\n",
    "            xin_n, delta_n = sample_batch(replay, batch_size)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xin_n)\n",
    "            loss = nn.functional.mse_loss(pred, delta_n)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            opt.step()\n",
    "            s += float(loss.item())\n",
    "        s /= iters\n",
    "        losses.append(s)\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {ep+1:03d} | loss={s:.6f} | lr={current_lr:.6f}\")\n",
    "    return losses\n",
    "\n",
    "losses = train_model(model, replay, epochs=10, batch_size=64)\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE (normalized Î”s)\")\n",
    "plt.title(\"Model Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1c7c0",
   "metadata": {},
   "source": [
    "## Task 5. Validate your model: One-Step and Multi-Step Prediction Error\n",
    "\n",
    "- Evaluate your trained model on a held-out set of random transitions.\n",
    "Generate a batch of unseen samples, predict the next-state delta, and compute the one-step MSE.\n",
    "\n",
    "- Repeat with open-loop rollouts of length k.\n",
    "Drive both the real environment and the model with the same action sequence, then report how prediction error grows across steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your code here\n",
    "\n",
    "def one_step_mse(model, replay, holdout=5000):\n",
    "    model.eval()\n",
    "    n = min(holdout, replay.size)\n",
    "    idx = np.random.choice(replay.size, size=n, replace=False)\n",
    "    s = replay.obs[idx]; a = replay.act[idx]; sp = replay.nxt[idx]\n",
    "    delta = sp - s\n",
    "    xin = np.concatenate([s, a], axis=1)\n",
    "\n",
    "    xin_n = to_t(inp_norm.normalize(xin))\n",
    "    pred_n = model(xin_n).detach().numpy()\n",
    "    pred = targ_norm.denormalize(pred_n)\n",
    "\n",
    "    mse = np.mean((pred - delta)**2)\n",
    "    return float(mse)\n",
    "\n",
    "mse_1 = one_step_mse(model, replay, holdout=5000)\n",
    "print(\"One-step MSE (Î”s):\", mse_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_step_open_loop_error(env, model, k=20, trials=10, seed=123):\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    errors = []\n",
    "\n",
    "    for _ in range(trials):\n",
    "        s, _ = env.reset(seed=int(rng.integers(0, 10_000)))\n",
    "        s_real = s.copy()\n",
    "        s_model = s.copy()\n",
    "\n",
    "        # fixed random actions to drive both systems identically\n",
    "        acts = [env.action_space.sample() for __ in range(k)]\n",
    "\n",
    "        err_sum = 0.0\n",
    "        for t in range(k):\n",
    "            a = acts[t]\n",
    "\n",
    "            # real env\n",
    "            sp_real, _, term, trunc, _ = env.step(a)\n",
    "            # model step\n",
    "            xin = np.concatenate([s_model[None, :], a[None, :]], axis=1)\n",
    "            xin_n = to_t(inp_norm.normalize(xin))\n",
    "            delta_n = model(xin_n).detach().numpy()\n",
    "            delta = targ_norm.denormalize(delta_n)[0]\n",
    "            sp_model = s_model + delta\n",
    "\n",
    "            err_sum += np.mean((sp_model - sp_real)**2)\n",
    "\n",
    "            s_real = sp_real\n",
    "            s_model = sp_model\n",
    "            if term or trunc:\n",
    "                break\n",
    "\n",
    "        errors.append(err_sum / max(1, t+1))\n",
    "\n",
    "    return float(np.mean(errors))\n",
    "\n",
    "mse_k = k_step_open_loop_error(env, model, k=20, trials=5)\n",
    "print(\"Avg k-step (k=20) open-loop MSE:\", mse_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a173d",
   "metadata": {},
   "source": [
    "\n",
    "## Task 6. Visualize Rollout Trajectories\n",
    "\n",
    "**Setup**\n",
    "Call model.eval() so gradients stay off.\n",
    "Reset the env with the provided seed; keep a copy of the initial observation.\n",
    "\n",
    "\n",
    "**Choose actions**\n",
    "Pre-sample k actions from env.action_space.sample() so the real system and the model rollout see the same sequence.\n",
    "\n",
    "**Roll forward**\n",
    "For each action:\n",
    "Step the real env (env.step(a)), append the new observation.\n",
    "For the model path:\n",
    "Build [s_model, a], normalize via inp_norm.normalize, turn into a tensor with to_t.\n",
    "Run the network, de-normalize with targ_norm.denormalize, add to the last model state, append.\n",
    "Stop early if the env terminates or truncates.\n",
    "\n",
    "**Plot**\n",
    "Plot the real trajectory as one line, model trajectory as another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_rollout(env, model, k=50, dims=(0, 5, 10), seed=2025):\n",
    "    model.eval()\n",
    "    s, _ = env.reset(seed=seed)\n",
    "    s_real = s.copy()\n",
    "    s_model = s.copy()\n",
    "    acts = [env.action_space.sample() for _ in range(k)]\n",
    "\n",
    "    traj_real = [s_real.copy()]\n",
    "    traj_model = [s_model.copy()]\n",
    "    for t in range(k):\n",
    "        a = acts[t]\n",
    "        sp_real, _, term, trunc, _ = env.step(a)\n",
    "\n",
    "        xin = np.concatenate([s_model[None, :], a[None, :]], axis=1)\n",
    "        xin_n = to_t(inp_norm.normalize(xin))\n",
    "        delta_n = model(xin_n).detach().numpy()\n",
    "        delta = targ_norm.denormalize(delta_n)[0]\n",
    "        sp_model = s_model + delta\n",
    "\n",
    "        traj_real.append(sp_real.copy())\n",
    "        traj_model.append(sp_model.copy())\n",
    "\n",
    "        s_real = sp_real\n",
    "        s_model = sp_model\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    traj_real = np.array(traj_real)\n",
    "    traj_model = np.array(traj_model)\n",
    "\n",
    "    print(dims)\n",
    "    for d in dims:\n",
    "        plt.figure()\n",
    "        plt.plot(traj_real[:, d], label=\"real\")\n",
    "        plt.plot(traj_model[:, d], label=\"model\")\n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(f\"state[{d}]\")\n",
    "        plt.title(f\"Dimension {d}: real vs model\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Uncomment to visualize\n",
    "dims = list(range(1, 11))\n",
    "visualize_rollout(env, model, k=50, dims=dims)#(qvel_start, qvel_start+1, qvel_start+2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f48f2",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Answer the questions : \n",
    "\n",
    "1. How good is your model? \n",
    "2. Is this training enough for planning, or do we need continual training?\n",
    "3. How is this system different from the mountain car problem? Why can't we learn this in one episode? \n",
    "4. Why do we use a runningnormalizer instead of a static normalizer? Think about the nature of the algorithm taught in class. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mquad_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
